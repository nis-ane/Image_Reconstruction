{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbff6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e06f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "data_flag = 'bloodmnist'\n",
    "# data_flag = 'breastmnist'\n",
    "download = False\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "lr = 0.005\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22cea466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "val_dataset = DataClass(split='val', download=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8e9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_train_dataset = DataClass(split = 'train', transform=data_transform)\n",
    "neu_val_dataset = DataClass(split = 'val', transform=data_transform)\n",
    "neu_test_dataset = DataClass(split = 'test', transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6e9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idx = (train_dataset.labels==6) \n",
    "neu_train_dataset.labels = train_dataset.labels[idx]\n",
    "neu_train_dataset.imgs = train_dataset.imgs[idx.squeeze()]\n",
    "\n",
    "idx = (val_dataset.labels==6) \n",
    "neu_val_dataset.labels = val_dataset.labels[idx]\n",
    "neu_val_dataset.imgs = val_dataset.imgs[idx.squeeze()]\n",
    "\n",
    "idx = (test_dataset.labels==6) \n",
    "neu_test_dataset.labels = test_dataset.labels[idx]\n",
    "neu_test_dataset.imgs = test_dataset.imgs[idx.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae02bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset=neu_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=neu_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=neu_test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1e908",
   "metadata": {},
   "source": [
    "# Simple VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caec60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, img_shape, code_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.C, self.H, self.W = img_shape\n",
    "        self.conv1 = nn.Conv2d(self.C, 32, kernel_size=3,  padding= \"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,  padding= \"same\")\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3,   padding= \"same\")\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding= \"same\")\n",
    "        self.fc1 = nn.Linear(256, code_size)\n",
    "        self.fc2 = nn.Linear(256, code_size)\n",
    "        \n",
    "\n",
    "        # decoder\n",
    "        self.fc3 = nn.Linear(code_size, 2304)\n",
    "        self.conv5 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv6 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "        self.conv7 = nn.ConvTranspose2d(64, self.C, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        stddev = torch.exp(0.5*log_var)\n",
    "        epsilon = torch.randn_like(stddev)\n",
    "        return mu + stddev*epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = F.elu(self.conv1(x))\n",
    "        m1 = F.max_pool2d(e1, kernel_size=2)\n",
    "        \n",
    "        e2 = F.elu(self.conv2(m1))\n",
    "        m2 = F.max_pool2d(e2, kernel_size=2)\n",
    "        \n",
    "        e3 = F.elu(self.conv3(m2))\n",
    "        m3 = F.max_pool2d(e3, kernel_size=2)\n",
    "        \n",
    "        e4 = F.elu(self.conv4(m3))\n",
    "        m4 = F.max_pool2d(e4, kernel_size=2)\n",
    "        \n",
    "        f1 = m4.view(m4.size(0), -1) \n",
    "        \n",
    "        mu = self.fc1(f1)\n",
    "        logvar = self.fc2(f1)\n",
    "        z = self.sampling(mu, logvar)\n",
    "        \n",
    "        d5 = self.fc3(z)\n",
    "        \n",
    "        d4 = d5.view(d5.size(0), 256, 3, 3)\n",
    "        d3 = F.elu(self.conv5(d4))\n",
    "        d2 = F.elu(self.conv6(d3))\n",
    "        d1 = self.conv7(d2)\n",
    "        return d1, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e04659",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 128\n",
    "vae = VAE(img_shape = (3,28,28),code_size  = z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a31a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(vae.parameters())\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e433bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDivLoss(mean, log_var):\n",
    "    \"\"\"\n",
    "    Implement KL div loss for Normal distribution with Standard Normal Distribution as reference probability distirbution\n",
    "\n",
    "    Params\n",
    "    ----------------------\n",
    "    mean: Tensor\n",
    "        mean tensor of candidate probability ditribution; shape: (Batch Size, Latent dim)\n",
    "    \n",
    "    log_var: Tensor\n",
    "        log variance of the candidate probability distribution; shape: (Batch Size, Latent dim)\n",
    "\n",
    "    Returns\n",
    "    -----------------------\n",
    "    Tensor\n",
    "        average KL divergence loss for the batch of inputs; shape: Scalar Tensor\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    ### BEGIN SOLUTION\n",
    "    loss =  -torch.mean(0.5 * torch.sum(1 + log_var - torch.square(mean) - torch.exp(log_var), dim = -1), dim = 0)\n",
    "    ### END SOLUTION\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9598159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_vae(epoch_index, model):\n",
    "    running_loss = 0.\n",
    "    running_recon_loss = 0.\n",
    "    running_kl_loss = 0.\n",
    "    last_loss = 0.\n",
    "    re_last_loss = 0.\n",
    "    kl_last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs, mu, logvar = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        recon_loss = loss_fn(outputs, inputs)\n",
    "        kl_loss = KLDivLoss(mu, logvar)\n",
    "        loss = recon_loss + 0.3*kl_loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_kl_loss += kl_loss.item()\n",
    "\n",
    "    last_loss = running_loss / len(train_loader) # loss per batch\n",
    "    re_last_loss = running_recon_loss / len(train_loader) # loss per batch\n",
    "    kl_last_loss = running_kl_loss / len(train_loader) # loss per batch\n",
    "    print('Epoch {} loss: {} recon_loss: {} kl_loss: {}'.format(epoch_index, last_loss, re_last_loss, kl_last_loss))\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_vae(model, out_path):\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    epoch_number = 0\n",
    "\n",
    "    N_EPOCHS = 80\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss  = train_one_epoch_vae(epoch_number, model)\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        model.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_re_vloss =0.0\n",
    "        running_kl_vloss = 0.0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs, vmu, vlog_var = model(vinputs)\n",
    "            \n",
    "            v_recon_loss = loss_fn(voutputs, vinputs)\n",
    "            v_kl_loss = KLDivLoss(vmu, vlog_var)\n",
    "            vloss = v_recon_loss + v_kl_loss\n",
    "            running_vloss += vloss\n",
    "            running_re_vloss += v_recon_loss\n",
    "            running_kl_vloss += v_kl_loss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        re_avg_rloss = running_re_vloss / (i + 1)\n",
    "        kl_avg_vloss = running_kl_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {} recon {} kl {}'.format(avg_loss, avg_vloss, re_avg_rloss, kl_avg_vloss))\n",
    "\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            torch.save(model.state_dict(), out_path)\n",
    "\n",
    "        epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53d2a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "Epoch 0 loss: 0.01622373432763023 recon_loss: 0.016218959168875464 kl_loss: 4.7751562032370406e-06\n",
      "LOSS train 0.01622373432763023 valid 0.01613292470574379 recon 0.016132591292262077 kl 3.3368638696629205e-07\n",
      "EPOCH 2:\n",
      "Epoch 1 loss: 0.016232094421268325 recon_loss: 0.0162318650607581 kl_loss: 2.2936561338202747e-07\n",
      "LOSS train 0.016232094421268325 valid 0.016120392829179764 recon 0.016120145097374916 kl 2.475475753271894e-07\n",
      "EPOCH 3:\n",
      "Epoch 2 loss: 0.016259203760642302 recon_loss: 0.016259024704308952 kl_loss: 1.7905633335244165e-07\n",
      "LOSS train 0.016259203760642302 valid 0.016167715191841125 recon 0.01616748981177807 kl 2.2607599703405867e-07\n",
      "EPOCH 4:\n",
      "Epoch 3 loss: 0.016259504845748618 recon_loss: 0.01625934435207754 kl_loss: 1.6049622260733524e-07\n",
      "LOSS train 0.016259504845748618 valid 0.01618223451077938 recon 0.01618204452097416 kl 1.911769800244656e-07\n",
      "EPOCH 5:\n",
      "Epoch 4 loss: 0.01620887178442862 recon_loss: 0.01620872818214232 kl_loss: 1.4360483782937778e-07\n",
      "LOSS train 0.01620887178442862 valid 0.01613605208694935 recon 0.016135862097144127 kl 1.9200255962914525e-07\n",
      "EPOCH 6:\n",
      "Epoch 5 loss: 0.016240333525897706 recon_loss: 0.016240190982512417 kl_loss: 1.4254083376035602e-07\n",
      "LOSS train 0.016240333525897706 valid 0.01616685651242733 recon 0.016166621819138527 kl 2.3668556536904362e-07\n",
      "EPOCH 7:\n",
      "Epoch 6 loss: 0.016250779061284783 recon_loss: 0.016250632907429785 kl_loss: 1.46151303467427e-07\n",
      "LOSS train 0.016250779061284783 valid 0.016208484768867493 recon 0.016208291053771973 kl 1.9179785226697277e-07\n",
      "EPOCH 8:\n",
      "Epoch 7 loss: 0.016197189286810486 recon_loss: 0.016197044485286898 kl_loss: 1.4480662674433716e-07\n",
      "LOSS train 0.016197189286810486 valid 0.01614340767264366 recon 0.01614321582019329 kl 1.9035823584090394e-07\n",
      "EPOCH 9:\n",
      "Epoch 8 loss: 0.0162565164740057 recon_loss: 0.01625634648849907 kl_loss: 1.6998550663255667e-07\n",
      "LOSS train 0.0162565164740057 valid 0.01616835780441761 recon 0.016168124973773956 kl 2.3162981221958034e-07\n",
      "EPOCH 10:\n",
      "Epoch 9 loss: 0.016196698294824934 recon_loss: 0.016196528143466334 kl_loss: 1.7014625553837167e-07\n",
      "LOSS train 0.016196698294824934 valid 0.016179915517568588 recon 0.0161796435713768 kl 2.7015860837309447e-07\n",
      "EPOCH 11:\n",
      "Epoch 10 loss: 0.016217896791353617 recon_loss: 0.0162176919003872 kl_loss: 2.0489096641540527e-07\n",
      "LOSS train 0.016217896791353617 valid 0.016668327152729034 recon 0.016667986288666725 kl 3.3846239944068657e-07\n",
      "EPOCH 12:\n",
      "Epoch 11 loss: 0.016216490609086538 recon_loss: 0.016216092768495213 kl_loss: 3.9784314285591555e-07\n",
      "LOSS train 0.016216490609086538 valid 0.016139093786478043 recon 0.016138728708028793 kl 3.5983165957986785e-07\n",
      "EPOCH 13:\n",
      "Epoch 12 loss: 0.016249039567598742 recon_loss: 0.016248707378870002 kl_loss: 3.321887287375045e-07\n",
      "LOSS train 0.016249039567598742 valid 0.016233215108513832 recon 0.016232561320066452 kl 6.51577863663988e-07\n",
      "EPOCH 14:\n",
      "Epoch 13 loss: 0.016187972299856683 recon_loss: 0.016185105771898 kl_loss: 2.8665266829188897e-06\n",
      "LOSS train 0.016187972299856683 valid 0.016112366691231728 recon 0.016112225130200386 kl 1.4201816611603135e-07\n",
      "EPOCH 15:\n",
      "Epoch 14 loss: 0.01621237599405728 recon_loss: 0.01621227339547995 kl_loss: 1.0259857732955723e-07\n",
      "LOSS train 0.01621237599405728 valid 0.016138164326548576 recon 0.01613806001842022 kl 1.0258194294010536e-07\n",
      "EPOCH 16:\n",
      "Epoch 15 loss: 0.016208029600906455 recon_loss: 0.016207943357884474 kl_loss: 8.624047039965985e-08\n",
      "LOSS train 0.016208029600906455 valid 0.016188355162739754 recon 0.01618824526667595 kl 1.0900227209731383e-07\n",
      "EPOCH 17:\n",
      "Epoch 16 loss: 0.016228385269641876 recon_loss: 0.016228304869712214 kl_loss: 8.040248123907139e-08\n",
      "LOSS train 0.016228385269641876 valid 0.016444560140371323 recon 0.01644446700811386 kl 9.603197526075746e-08\n",
      "EPOCH 18:\n",
      "Epoch 17 loss: 0.0162047295167736 recon_loss: 0.016204649193391 kl_loss: 8.032338259971304e-08\n",
      "LOSS train 0.0162047295167736 valid 0.01606789417564869 recon 0.016067801043391228 kl 9.434672421093637e-08\n",
      "EPOCH 19:\n",
      "Epoch 18 loss: 0.016206566971560865 recon_loss: 0.01620648447934487 kl_loss: 8.249476757108425e-08\n",
      "LOSS train 0.016206566971560865 valid 0.01643415167927742 recon 0.01643405109643936 kl 1.0065789268765002e-07\n",
      "EPOCH 20:\n",
      "Epoch 19 loss: 0.016195934221199523 recon_loss: 0.016195849145520223 kl_loss: 8.507440353406771e-08\n",
      "LOSS train 0.016195934221199523 valid 0.016143258661031723 recon 0.016143152490258217 kl 1.0536567884855685e-07\n",
      "EPOCH 21:\n",
      "Epoch 20 loss: 0.01625338135516807 recon_loss: 0.016253277161860303 kl_loss: 1.0419585934397368e-07\n",
      "LOSS train 0.01625338135516807 valid 0.016168799251317978 recon 0.01616869680583477 kl 1.042671868844991e-07\n",
      "EPOCH 22:\n",
      "Epoch 21 loss: 0.016215378169786847 recon_loss: 0.01621525750610314 kl_loss: 1.2066368370839995e-07\n",
      "LOSS train 0.016215378169786847 valid 0.016258062794804573 recon 0.016257917508482933 kl 1.4617329213706398e-07\n",
      "EPOCH 23:\n",
      "Epoch 22 loss: 0.016185661860779948 recon_loss: 0.01618552440139529 kl_loss: 1.374581088953164e-07\n",
      "LOSS train 0.016185661860779948 valid 0.016099024564027786 recon 0.0160988662391901 kl 1.5786770291015273e-07\n",
      "EPOCH 24:\n",
      "Epoch 23 loss: 0.016227019829547978 recon_loss: 0.016226860560629875 kl_loss: 1.5926381494587264e-07\n",
      "LOSS train 0.016227019829547978 valid 0.016166454181075096 recon 0.01616625487804413 kl 1.9864121725277073e-07\n",
      "EPOCH 25:\n",
      "Epoch 24 loss: 0.016154026593502662 recon_loss: 0.01615382109015976 kl_loss: 2.0550844605968575e-07\n",
      "LOSS train 0.016154026593502662 valid 0.016128120943903923 recon 0.01612791232764721 kl 2.0810453804642748e-07\n",
      "EPOCH 26:\n",
      "Epoch 25 loss: 0.01620096461975003 recon_loss: 0.01619881001135258 kl_loss: 2.154613500510693e-06\n",
      "LOSS train 0.01620096461975003 valid 0.01612897403538227 recon 0.0161284226924181 kl 5.536013532037032e-07\n",
      "EPOCH 27:\n",
      "Epoch 26 loss: 0.01618079320616918 recon_loss: 0.01618068486018336 kl_loss: 1.0834726158573051e-07\n",
      "LOSS train 0.01618079320616918 valid 0.016182713210582733 recon 0.016182633116841316 kl 7.974577442837472e-08\n",
      "EPOCH 28:\n",
      "Epoch 27 loss: 0.01619971081716557 recon_loss: 0.016199648284595716 kl_loss: 6.253384564507535e-08\n",
      "LOSS train 0.01619971081716557 valid 0.016153426840901375 recon 0.0161533635109663 kl 6.288644982532787e-08\n",
      "EPOCH 29:\n",
      "Epoch 28 loss: 0.01619853396351411 recon_loss: 0.016198476489429195 kl_loss: 5.747918804544254e-08\n",
      "LOSS train 0.01619853396351411 valid 0.01609351858496666 recon 0.016093458980321884 kl 5.978203887480049e-08\n",
      "EPOCH 30:\n",
      "Epoch 29 loss: 0.01616917248840814 recon_loss: 0.016169114988807538 kl_loss: 5.7494497443027806e-08\n",
      "LOSS train 0.01616917248840814 valid 0.01622776687145233 recon 0.01622770167887211 kl 6.729402457494871e-08\n",
      "EPOCH 31:\n",
      "Epoch 30 loss: 0.016230206247674277 recon_loss: 0.016230147714688354 kl_loss: 5.853553750083947e-08\n",
      "LOSS train 0.016230206247674277 valid 0.016142098233103752 recon 0.01614202931523323 kl 7.11421250798594e-08\n",
      "EPOCH 32:\n",
      "Epoch 31 loss: 0.01620130739110994 recon_loss: 0.016201245081802346 kl_loss: 6.230675601639083e-08\n",
      "LOSS train 0.01620130739110994 valid 0.01616767607629299 recon 0.01616760343313217 kl 7.320263506471747e-08\n",
      "EPOCH 33:\n",
      "Epoch 32 loss: 0.016195361260069558 recon_loss: 0.016195293834866727 kl_loss: 6.742520283346307e-08\n",
      "LOSS train 0.016195361260069558 valid 0.016119273379445076 recon 0.016119202598929405 kl 7.292972270533937e-08\n",
      "EPOCH 34:\n",
      "Epoch 33 loss: 0.016213079410515828 recon_loss: 0.016212999335911176 kl_loss: 8.007588041658119e-08\n",
      "LOSS train 0.016213079410515828 valid 0.016106894239783287 recon 0.01610681042075157 kl 8.224976966175745e-08\n",
      "EPOCH 35:\n",
      "Epoch 34 loss: 0.016184407981648427 recon_loss: 0.016184308904235903 kl_loss: 9.907230937003938e-08\n",
      "LOSS train 0.016184407981648427 valid 0.016116321086883545 recon 0.016116194427013397 kl 1.242855063310344e-07\n",
      "EPOCH 36:\n",
      "Epoch 35 loss: 0.016189572656501647 recon_loss: 0.01618943067446147 kl_loss: 1.4198204017665287e-07\n",
      "LOSS train 0.016189572656501647 valid 0.016150962561368942 recon 0.016150735318660736 kl 2.2508667996135046e-07\n",
      "EPOCH 37:\n",
      "Epoch 36 loss: 0.016160969265213567 recon_loss: 0.016159474390418562 kl_loss: 1.494869691848884e-06\n",
      "LOSS train 0.016160969265213567 valid 0.01611635833978653 recon 0.016116280108690262 kl 8.093977754697335e-08\n",
      "EPOCH 38:\n",
      "Epoch 37 loss: 0.016183828188693277 recon_loss: 0.01618377223279174 kl_loss: 5.595845311628705e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.016183828188693277 valid 0.01609218120574951 recon 0.01609213277697563 kl 4.8858613155289277e-08\n",
      "EPOCH 39:\n",
      "Epoch 38 loss: 0.016194592969976877 recon_loss: 0.01619454331645002 kl_loss: 4.9653526854841675e-08\n",
      "LOSS train 0.016194592969976877 valid 0.01631886512041092 recon 0.01631881296634674 kl 5.2631669689162663e-08\n",
      "EPOCH 40:\n",
      "Epoch 39 loss: 0.01621556103433648 recon_loss: 0.016215512382300342 kl_loss: 4.8650760351827324e-08\n",
      "LOSS train 0.01621556103433648 valid 0.016129253432154655 recon 0.016129205003380775 kl 4.998438996040022e-08\n",
      "EPOCH 41:\n",
      "Epoch 40 loss: 0.01619206316616029 recon_loss: 0.016192016332116844 kl_loss: 4.6836595022450513e-08\n",
      "LOSS train 0.01619206316616029 valid 0.0160902701318264 recon 0.01609022356569767 kl 4.665482578047886e-08\n",
      "EPOCH 42:\n",
      "Epoch 41 loss: 0.016185006337265852 recon_loss: 0.016184954680757573 kl_loss: 5.166161141496777e-08\n",
      "LOSS train 0.016185006337265852 valid 0.016051918268203735 recon 0.016051873564720154 kl 4.5508581791864344e-08\n",
      "EPOCH 43:\n",
      "Epoch 42 loss: 0.016185135848514023 recon_loss: 0.016185071880686772 kl_loss: 6.39678272482467e-08\n",
      "LOSS train 0.016185135848514023 valid 0.016415193676948547 recon 0.01641511544585228 kl 7.572027982405416e-08\n",
      "EPOCH 44:\n",
      "Epoch 43 loss: 0.016188361535282577 recon_loss: 0.016188281658424498 kl_loss: 7.98743064992061e-08\n",
      "LOSS train 0.016188361535282577 valid 0.01631263829767704 recon 0.016312533989548683 kl 1.0275251582925193e-07\n",
      "EPOCH 45:\n",
      "Epoch 44 loss: 0.01619962087436898 recon_loss: 0.01619946654273631 kl_loss: 1.5432908114316017e-07\n",
      "LOSS train 0.01619962087436898 valid 0.016128385439515114 recon 0.016128137707710266 kl 2.4784779384390276e-07\n",
      "EPOCH 46:\n",
      "Epoch 45 loss: 0.016187288377382984 recon_loss: 0.016187095555336508 kl_loss: 1.9282714963272977e-07\n",
      "LOSS train 0.016187288377382984 valid 0.01610623300075531 recon 0.016106003895401955 kl 2.3272147586794745e-07\n",
      "EPOCH 47:\n",
      "Epoch 46 loss: 0.016183550960754287 recon_loss: 0.016182710257141967 kl_loss: 8.406985091641816e-07\n",
      "LOSS train 0.016183550960754287 valid 0.016164440661668777 recon 0.0161643885076046 kl 5.001850666985774e-08\n",
      "EPOCH 48:\n",
      "Epoch 47 loss: 0.01617221484496577 recon_loss: 0.0161721645407889 kl_loss: 5.030672845115476e-08\n",
      "LOSS train 0.01617221484496577 valid 0.01613551378250122 recon 0.01613546349108219 kl 5.106922529307667e-08\n",
      "EPOCH 49:\n",
      "Epoch 48 loss: 0.01616817663666116 recon_loss: 0.016168129904680466 kl_loss: 4.673453227454745e-08\n",
      "LOSS train 0.01616817663666116 valid 0.01620243489742279 recon 0.016202380880713463 kl 5.055068896808734e-08\n",
      "EPOCH 50:\n",
      "Epoch 49 loss: 0.016169032483833702 recon_loss: 0.0161689772423714 kl_loss: 5.5244013880965586e-08\n",
      "LOSS train 0.016169032483833702 valid 0.01623406447470188 recon 0.016233986243605614 kl 7.689381220643554e-08\n",
      "EPOCH 51:\n",
      "Epoch 50 loss: 0.016200859450467238 recon_loss: 0.01620079878692145 kl_loss: 6.065844262802285e-08\n",
      "LOSS train 0.016200859450467238 valid 0.016262337565422058 recon 0.016262274235486984 kl 6.52267004852547e-08\n",
      "EPOCH 52:\n",
      "Epoch 51 loss: 0.016166828336729985 recon_loss: 0.0161667472542557 kl_loss: 8.108375005212397e-08\n",
      "LOSS train 0.016166828336729985 valid 0.01610432006418705 recon 0.016104236245155334 kl 8.113082117233716e-08\n",
      "EPOCH 53:\n",
      "Epoch 52 loss: 0.016182216094832304 recon_loss: 0.016181996270810087 kl_loss: 2.1982529769085065e-07\n",
      "LOSS train 0.016182216094832304 valid 0.016080137342214584 recon 0.01607704535126686 kl 3.09238657791866e-06\n",
      "EPOCH 54:\n",
      "Epoch 53 loss: 0.01617449734436527 recon_loss: 0.016173546999846012 kl_loss: 9.503419676774814e-07\n",
      "LOSS train 0.01617449734436527 valid 0.0162302628159523 recon 0.01623021811246872 kl 4.293636024499392e-08\n",
      "EPOCH 55:\n",
      "Epoch 54 loss: 0.016191501508479658 recon_loss: 0.016191457940444146 kl_loss: 4.357058708042728e-08\n",
      "LOSS train 0.016191501508479658 valid 0.016094351187348366 recon 0.016094308346509933 kl 4.5433530715399684e-08\n",
      "EPOCH 56:\n",
      "Epoch 55 loss: 0.016149350920411414 recon_loss: 0.01614930745443865 kl_loss: 4.3465972763218294e-08\n",
      "LOSS train 0.016149350920411414 valid 0.016175081953406334 recon 0.016175037249922752 kl 4.503780459685913e-08\n",
      "EPOCH 57:\n",
      "Epoch 56 loss: 0.016157675521728927 recon_loss: 0.01615763186438851 kl_loss: 4.3654788837105504e-08\n",
      "LOSS train 0.016157675521728927 valid 0.01612117886543274 recon 0.016121134161949158 kl 4.4635253715341605e-08\n",
      "EPOCH 58:\n",
      "Epoch 57 loss: 0.016186991336513055 recon_loss: 0.01618694781950892 kl_loss: 4.35119010046414e-08\n",
      "LOSS train 0.016186991336513055 valid 0.016137724742293358 recon 0.01613767072558403 kl 4.9083773490110616e-08\n",
      "EPOCH 59:\n",
      "Epoch 58 loss: 0.01617631051459745 recon_loss: 0.016176259138761726 kl_loss: 5.137583572570592e-08\n",
      "LOSS train 0.01617631051459745 valid 0.01613685116171837 recon 0.016136785969138145 kl 6.436701482925855e-08\n",
      "EPOCH 60:\n",
      "Epoch 59 loss: 0.01618597573562436 recon_loss: 0.0161859120867432 kl_loss: 6.365143273948066e-08\n",
      "LOSS train 0.01618597573562436 valid 0.01609002612531185 recon 0.01608995720744133 kl 6.929312945658239e-08\n",
      "EPOCH 61:\n",
      "Epoch 60 loss: 0.016260666319819754 recon_loss: 0.016260575088480972 kl_loss: 9.123389031161697e-08\n",
      "LOSS train 0.016260666319819754 valid 0.016139904037117958 recon 0.01613977923989296 kl 1.21638237260413e-07\n",
      "EPOCH 62:\n",
      "Epoch 61 loss: 0.016178325341682728 recon_loss: 0.016177990614143135 kl_loss: 3.347275395915933e-07\n",
      "LOSS train 0.016178325341682728 valid 0.016099993139505386 recon 0.016099894419312477 kl 9.865195949032568e-08\n",
      "EPOCH 63:\n",
      "Epoch 62 loss: 0.016194263326437915 recon_loss: 0.016194204487263747 kl_loss: 5.884427729864605e-08\n",
      "LOSS train 0.016194263326437915 valid 0.01623605750501156 recon 0.016235999763011932 kl 5.583841655720789e-08\n",
      "EPOCH 64:\n",
      "Epoch 63 loss: 0.016168791787979538 recon_loss: 0.016168731162707284 kl_loss: 6.062782383285233e-08\n",
      "LOSS train 0.016168791787979538 valid 0.016109002754092216 recon 0.016108939424157143 kl 5.781705070262433e-08\n",
      "EPOCH 65:\n",
      "Epoch 64 loss: 0.016173860804522284 recon_loss: 0.01617231133891499 kl_loss: 1.5494681588566627e-06\n",
      "LOSS train 0.016173860804522284 valid 0.016188321635127068 recon 0.01618826761841774 kl 5.359369126267666e-08\n",
      "EPOCH 66:\n",
      "Epoch 65 loss: 0.01620798394696353 recon_loss: 0.016207938210094627 kl_loss: 4.5741972036589935e-08\n",
      "LOSS train 0.01620798394696353 valid 0.016138089820742607 recon 0.016138048842549324 kl 4.1865167332844067e-08\n",
      "EPOCH 67:\n",
      "Epoch 66 loss: 0.0161717782970773 recon_loss: 0.016171736132404576 kl_loss: 4.216212114902316e-08\n",
      "LOSS train 0.0161717782970773 valid 0.016131015494465828 recon 0.016130970790982246 kl 4.3120575554667084e-08\n",
      "EPOCH 68:\n",
      "Epoch 67 loss: 0.01615558051797625 recon_loss: 0.01615553910601629 kl_loss: 4.140685682914066e-08\n",
      "LOSS train 0.01615558051797625 valid 0.01611427403986454 recon 0.016114234924316406 kl 4.346854254322352e-08\n",
      "EPOCH 69:\n",
      "Epoch 68 loss: 0.016179697052256702 recon_loss: 0.016179656163368323 kl_loss: 4.0888888378665874e-08\n",
      "LOSS train 0.016179697052256702 valid 0.01610499806702137 recon 0.016104957088828087 kl 4.31615134743879e-08\n",
      "EPOCH 70:\n",
      "Epoch 69 loss: 0.016146311096285712 recon_loss: 0.01614626759203942 kl_loss: 4.350934942621036e-08\n",
      "LOSS train 0.016146311096285712 valid 0.01612209714949131 recon 0.016122041270136833 kl 5.523118318251363e-08\n",
      "EPOCH 71:\n",
      "Epoch 70 loss: 0.0161553473364919 recon_loss: 0.016155301867537712 kl_loss: 4.546895419081596e-08\n",
      "LOSS train 0.0161553473364919 valid 0.016064908355474472 recon 0.01606486178934574 kl 4.464889968858188e-08\n",
      "EPOCH 72:\n",
      "Epoch 71 loss: 0.016146812332819584 recon_loss: 0.016146757486850433 kl_loss: 5.485107231127224e-08\n",
      "LOSS train 0.016146812332819584 valid 0.01615099422633648 recon 0.016150925308465958 kl 6.695970711234622e-08\n",
      "EPOCH 73:\n",
      "Epoch 72 loss: 0.016206402739841645 recon_loss: 0.016206339677821282 kl_loss: 6.306202036060698e-08\n",
      "LOSS train 0.016206402739841645 valid 0.016082290560007095 recon 0.01608223095536232 kl 6.180160738722407e-08\n",
      "EPOCH 74:\n",
      "Epoch 73 loss: 0.016174435845180732 recon_loss: 0.016174357384443283 kl_loss: 7.845818587205105e-08\n",
      "LOSS train 0.016174435845180732 valid 0.01605946570634842 recon 0.01605937071144581 kl 9.506313602969385e-08\n",
      "EPOCH 75:\n",
      "Epoch 74 loss: 0.016157141446505915 recon_loss: 0.016157035594678498 kl_loss: 1.0585693057582954e-07\n",
      "LOSS train 0.016157141446505915 valid 0.0161555428057909 recon 0.0161554254591465 kl 1.1787882669977989e-07\n",
      "EPOCH 76:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 loss: 0.016167898962197648 recon_loss: 0.016167785245159716 kl_loss: 1.1371576216436564e-07\n",
      "LOSS train 0.016167898962197648 valid 0.016103528439998627 recon 0.016103439033031464 kl 9.009607992993551e-08\n",
      "EPOCH 77:\n",
      "Epoch 76 loss: 0.016174312738369997 recon_loss: 0.016174155230034296 kl_loss: 1.5750323254454475e-07\n",
      "LOSS train 0.016174312738369997 valid 0.01608477346599102 recon 0.016084667295217514 kl 1.0311412523833496e-07\n",
      "EPOCH 78:\n",
      "Epoch 77 loss: 0.01617382681124831 recon_loss: 0.016173499789446183 kl_loss: 3.270218021249118e-07\n",
      "LOSS train 0.01617382681124831 valid 0.016162361949682236 recon 0.016162309795618057 kl 5.411905590335664e-08\n",
      "EPOCH 79:\n",
      "Epoch 78 loss: 0.01617053567674266 recon_loss: 0.016170492867798838 kl_loss: 4.281021961280782e-08\n",
      "LOSS train 0.01617053567674266 valid 0.016160158440470695 recon 0.016160115599632263 kl 4.2567922520220236e-08\n",
      "EPOCH 80:\n",
      "Epoch 79 loss: 0.016127806316346745 recon_loss: 0.016127762639869567 kl_loss: 4.3677752969983885e-08\n",
      "LOSS train 0.016127806316346745 valid 0.01616668328642845 recon 0.01616663485765457 kl 4.724159197166955e-08\n"
     ]
    }
   ],
   "source": [
    "train_vae(vae, 'simple_vae_pytorch_model_128_code_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bbd2f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = VAE(img_shape = (3,28,28),code_size  = z_dim) \n",
    "saved_model.load_state_dict(torch.load('simple_vae_pytorch_model_128_code_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b82dacfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS test 0.016369644552469254\n"
     ]
    }
   ],
   "source": [
    "running_tloss = 0.0\n",
    "for i, tdata in enumerate(test_loader):\n",
    "    tinputs, tlabels = tdata\n",
    "    toutputs, tmu, tlog_var = saved_model(tinputs)\n",
    "    \n",
    "    t_recon_loss = loss_fn(toutputs, tinputs)\n",
    "    t_kl_loss = KLDivLoss(tmu, tlog_var)\n",
    "    tloss = t_recon_loss + t_kl_loss\n",
    "    running_tloss += t_recon_loss\n",
    "\n",
    "\n",
    "avg_tloss = running_tloss / (i + 1)\n",
    "print('LOSS test {}'.format(avg_tloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc3d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.5658e-07, -2.0267e-05, -1.4858e-05,  2.7113e-05,  2.2572e-05,\n",
      "        -7.2569e-06,  5.2884e-05, -4.6217e-07,  7.9740e-06, -7.1738e-05,\n",
      "         2.6572e-05, -4.3908e-05, -1.0298e-04, -1.7012e-05, -5.6407e-05,\n",
      "         1.0053e-05,  6.1206e-05, -2.5914e-05,  4.2375e-08, -2.3244e-05,\n",
      "        -3.3099e-05,  1.5274e-06,  3.5182e-05, -6.3777e-06, -3.7421e-05,\n",
      "         3.7553e-05,  3.8314e-05,  2.5761e-05, -1.8742e-05,  1.5873e-05,\n",
      "        -3.7264e-05, -5.3291e-06,  2.3870e-05,  3.0361e-07, -3.2989e-05,\n",
      "         1.2413e-05,  6.3664e-05,  1.3662e-05,  6.1456e-05, -1.9202e-05,\n",
      "         1.5099e-07,  5.0301e-05,  1.1551e-05, -1.9293e-05,  7.1339e-07,\n",
      "        -9.7314e-06,  1.2234e-05,  3.3293e-05,  1.4491e-06, -5.3674e-05,\n",
      "        -8.0692e-05,  1.9157e-06,  9.4767e-05,  1.7977e-05, -1.6232e-05,\n",
      "         4.5927e-05,  3.1893e-06,  2.2990e-05, -6.9588e-06, -3.7525e-05,\n",
      "        -3.2461e-05, -1.9683e-05, -2.0508e-06,  2.6669e-05,  1.7257e-05,\n",
      "         2.7000e-05,  1.9521e-06,  3.6946e-05, -4.6364e-05,  3.4774e-05,\n",
      "        -4.2332e-05, -2.2043e-05,  9.7603e-06,  6.1076e-06,  3.6091e-05,\n",
      "         9.9372e-06, -2.5684e-05,  2.5385e-05, -1.2507e-05,  1.5093e-05,\n",
      "         3.2328e-05,  2.4866e-07, -9.9838e-07, -1.9602e-05, -2.6146e-05,\n",
      "         9.9046e-07, -3.3345e-05, -5.9729e-05,  6.2563e-05,  7.2084e-06,\n",
      "        -6.1840e-06, -1.8163e-05,  2.2318e-05,  3.0672e-05,  2.1629e-05,\n",
      "         1.5979e-05,  3.4612e-05,  1.3958e-05, -1.0855e-05,  2.1941e-05,\n",
      "        -2.0800e-05, -4.5428e-05, -6.8906e-06, -3.1896e-05, -1.8580e-06,\n",
      "        -4.2595e-05,  4.6608e-05, -7.7514e-06, -2.5153e-05, -1.5753e-05,\n",
      "         3.3705e-05,  1.9955e-05,  1.7587e-05, -1.3101e-05, -7.0732e-06,\n",
      "         1.0734e-05, -9.9177e-06, -2.6116e-05,  1.0142e-05,  2.6915e-05,\n",
      "         5.4892e-05,  4.0458e-05, -3.8140e-05,  3.1257e-05, -7.9448e-06,\n",
      "         9.8888e-06,  3.7159e-05,  4.1495e-05], grad_fn=<SelectBackward0>) tensor([ 2.4419e-05, -3.4321e-05,  1.5553e-06, -1.6615e-06,  1.0964e-05,\n",
      "         1.0773e-05,  4.5456e-05, -1.7416e-05, -3.4319e-06, -5.9612e-05,\n",
      "         3.7325e-06, -6.2935e-05, -7.3854e-05, -2.6773e-05, -3.5456e-05,\n",
      "         1.1756e-05,  6.4831e-05, -2.9306e-05, -1.4732e-05, -9.3589e-06,\n",
      "         3.0976e-06,  5.4631e-06,  2.5269e-05, -2.0390e-05, -4.3234e-05,\n",
      "         1.5659e-05,  1.2930e-05,  1.3399e-05, -5.2992e-06,  3.9005e-05,\n",
      "        -2.1858e-05,  1.6136e-05,  2.3099e-05,  1.3497e-05, -1.6322e-05,\n",
      "        -5.8860e-07,  6.7595e-05,  3.2788e-05,  7.8306e-05, -5.4138e-06,\n",
      "         2.4943e-05,  4.6492e-05,  3.5382e-05, -3.6205e-05,  1.0956e-05,\n",
      "        -2.0911e-05,  1.2863e-05,  2.5569e-05,  2.7165e-05, -7.4953e-05,\n",
      "        -8.4586e-05, -5.2834e-06,  1.0538e-04,  4.2264e-05, -4.4351e-05,\n",
      "         5.7732e-06, -7.0171e-06,  3.3764e-05, -5.2882e-05, -4.7540e-05,\n",
      "        -4.5014e-05, -2.3684e-05, -1.4577e-05,  2.1459e-05,  1.4048e-05,\n",
      "         4.6089e-05,  8.0969e-06,  2.0606e-05, -8.7508e-05,  2.0675e-07,\n",
      "        -1.0140e-05, -1.0094e-05, -1.7043e-07, -1.2545e-06,  3.3524e-05,\n",
      "        -8.4713e-06, -3.1586e-05,  2.1365e-05, -1.7781e-05,  7.5006e-06,\n",
      "         1.8068e-05, -7.5437e-06, -1.5968e-05, -2.6499e-05, -3.4355e-05,\n",
      "        -5.7500e-06, -7.2679e-05, -5.8185e-05,  1.8550e-05,  6.9849e-06,\n",
      "        -1.6574e-05, -2.5734e-05,  2.0707e-05, -1.7341e-05,  2.2345e-05,\n",
      "        -1.7913e-05,  2.3462e-05, -2.4362e-05,  3.2287e-05,  3.7154e-05,\n",
      "        -1.0574e-05, -3.5584e-05, -2.3067e-05, -4.7242e-05, -3.1626e-05,\n",
      "        -6.0797e-05,  3.0486e-05, -2.0284e-05, -1.7325e-05,  4.3400e-06,\n",
      "         2.4971e-05,  1.3785e-05,  1.8775e-06, -4.6883e-06,  1.9681e-05,\n",
      "         2.7297e-05,  1.4534e-05, -1.6997e-05,  1.5816e-05,  4.5279e-05,\n",
      "        -2.4531e-06,  3.9462e-05, -2.9574e-05,  1.0847e-05,  2.3561e-05,\n",
      "        -2.4116e-05,  4.7802e-05,  4.6672e-05], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYbUlEQVR4nO2dW4ycZ3nH/8+c9rzek72x1yZ2jCENJ4cu6SEtpUJFITcBVVTkAoKKai5AAomLInpBLqOqgLioqEyJCBUNQgJELqKWKEVCVCqKidKcDDgmTrzetb327npPs3P6nl7s0Jqw7//d7OzOTHn/P2m1u/PM+33vd/jPNzP/73kec3cIIX73yXV6AkKI9iCxC5EIErsQiSCxC5EIErsQiVBo58omxkb86JFDLSzBgpGYq1CvNWi8VqvReKPBxzPM+GtqzBDJ5fj4QiF8GItFfohjy2b7fDthWJ0E+Ya7x/Y5H29GJsdiAOCReBaJ5/h+93p47lnGt4vFZy7P4vrS0paTa0nsZnYPgK8AyAP4Z3d/iD3/6JFDeOrxb7Ll8RWSHViv8BNj/uoijc/OXqbxG0urwZhHToxioZfGYy9Uvb18/P79+4OxAwcO0LEDAwM0bvk8jUffG5bmg6Esq9Kh9cYajRsqNF4ohOduhSIdi0aJhrMNHs+V+H6vXC8HY+tr7AUSqFbD++19f/3R8JzoUglmlgfwjwDeD+AOAPeb2R07XZ4QYm9p5TP7XQBecvdfuXsVwLcB3Lc70xJC7DatiH0KwMWb/p9pPvYbmNkpMztjZmfmr/O30kKIvaMVsW/1QfW3Pny6+2l3n3b36f3joy2sTgjRCq2IfQbAkZv+PwxgtrXpCCH2ilbE/hSAE2Z2zMxKAD4M4LHdmZYQYrfZsfXm7nUz+xSAf8em9fawu78QHxl+fYn5i5ZlwVg+YhEN7+un8cwnaHxoKGxRZWReALC+vk7jfX19ND42PkLj4+NjwVihN/J63ghbigBQr3F7i/nFAODMj45Yrbl8D40XYvYZiIUVm3edH9PY/QnVlSUav3ZtORi7scTPl2o1vF3sfpGWfHZ3fxzA460sQwjRHnS7rBCJILELkQgSuxCJILELkQgSuxCJILELkQhtzWcHDGZhbzRr8NQ+B/HZC/x1a2CIe9k9ET+6Wg2nmTYyngtfKo3TOIx7umaR/GZcC8bKZT63GH19PL0Wg4M0XF8Mx7PIfsvn+PkARLbNw6mgtUo4xXRzLD8mxSI/X2L3Viwuh/NEWDo1ADRYLjzRkK7sQiSCxC5EIkjsQiSCxC5EIkjsQiSCxC5EIrTZegPY60ssbZBXYeVWSczeKvBioSiUwim0sVLQmXOLKGat5SI2D1i56IyPbWxwC2qjcoPGrRpO1QSAkt8VjNU2ePXYunH7qrefb5vlSOnxRszujNXI5inV5TLftlqdpQ5HbL9S+HizbdaVXYhEkNiFSASJXYhEkNiFSASJXYhEkNiFSASJXYhEaK/P7g4nnTstz71NFvVIumRWj3jduYjXnQt7nxZpdJoHL3lc2dig8XqZdzstFcNpqIXIDQT1Kt/nayvc665UeKnp+mrYb67WuBedL/LtHh3n27aPlA8vRVpZZ1kkvTZSanplhd+fsEHuMchFyqL39oS3m90foCu7EIkgsQuRCBK7EIkgsQuRCBK7EIkgsQuRCBK7EInQ5nx2R+ZhXzafixjWJN/dIr4nIm2VuYsPwImn24iMNe4Hl1e4x7+4wD3fKvG6PWvQsfUan3u1wk+Rep1fL14+94tgrJDn2zW0j6/70JFhGncSHxnjpcVzOe7xx1pZs9bJAJA5Kfkcud+kVArft8Hy2VsSu5ldALACoAGg7u7TrSxPCLF37MaV/c/dPdylQAjRFegzuxCJ0KrYHcAPzexnZnZqqyeY2SkzO2NmZ+YXllpcnRBip7Qq9rvd/Z0A3g/gk2b27tc+wd1Pu/u0u0/vHxtpcXVCiJ3Sktjdfbb5+yqA7wMIlxIVQnSUHYvdzAbMbOjXfwN4H4Dnd2tiQojdpZVv4ycBfL+ZP1sA8K/u/m90hDnPUc7xvG+Gg3u2+diyIx5/Vgu/Lsa6/1589SqNe6OHxhs13vK5vBae27WrPGd85lVupMxfjeVlcz95fS18vAcG+ek3PMKP2fz8dRpfXd0fjL3xTZN07NgYb1Wdi9T67+vjPv7Kykow5s7vjfA8j4fYsdjd/VcA3rHT8UKI9iLrTYhEkNiFSASJXYhEkNiFSASJXYhEaG+KqzlQZJZFpHwvabPrHilDHWkHDeNWS70anvf6KrdCyqvcQspq4ZLHALC+wg/T7EzYxvn5i7N07MvnuS0Y27Zige83K64GY4ND3HJcXgqPBYDF69z+atTCtt/w4AAdOzzE46V+fkx6evi2sdLl1Ugb7Wo1HHcPa0RXdiESQWIXIhEkdiESQWIXIhEkdiESQWIXIhEkdiESoe0tm7Mqadkc6X3s1Fbl5ZphkU2NlEReXAinel58ZZGOLa/ydfeW+HY36nz8k0/8ZzBWKfOxI8NTNN5X4vcvXL+2xJc/MBGMnf/FL+nY4ycO0fjCwhUeXwzHR8a4j56LlLm+/a430/jo6CiNb1TCqceXL/PtWl5ZCMYaDVKimi5VCPE7g8QuRCJI7EIkgsQuRCJI7EIkgsQuRCJI7EIkQptbNhtytKQz95vNyGuTR0pFR3z01RWeQ7x8I+yLbmxs0LGZ85xv5psCABrcEx4cDm/b1flLdGwsPjjA/eKB4SEar66HSyoP9Y3RsbUKP2ae8ZzxxevhfPgbi7zl8voab/FduR6uIQAApRKvUcBKTQ8M8TLU1Uq4xgBr96wruxCJILELkQgSuxCJILELkQgSuxCJILELkQgSuxCJ0GafPQeAeKMZz52GMS+db0q5TFpFA7h+jbcmXlwMxysVvuxGjddeN/B4PnKUxifD+63Ux33yQweP0HisHv8LL5yl8Y3ysWBs3/AIHZvLeF343gL3+CvlpWBs4Rq/r8IzXh9ho8x9+H3DgzQ+Oh6+xyBX4Ntdq4XbZBeLYY1Er+xm9rCZXTWz5296bMzMnjCzc83f/IwSQnSc7byN/waAe17z2OcAPOnuJwA82fxfCNHFRMXu7j8G8Nr7Oe8D8Ejz70cAfGB3pyWE2G12+gXdpLvPAUDz94HQE83slJmdMbMz8wu8VpsQYu/Y82/j3f20u0+7+/T+MX20F6JT7FTsV8zsIAA0f/NWoEKIjrNTsT8G4IHm3w8A+MHuTEcIsVdEfXYzexTAewBMmNkMgC8AeAjAd8zs4wBeBfCh3VilR3xVePi1yTOeCx/rM35jieekr6+FvU043435PJ9bPs9fc/t6+fLv/P0Twdjw8DAd+/Z3vJXG5+bmaHx+6VkaPz8b3m9OjicAOLiXnQe/B6BYCNcBuPQKfzPaqL2DxjfK/HzaF6nNwPLZ84UROjbLwvulWAivNyp2d78/EHpvbKwQonvQ7bJCJILELkQiSOxCJILELkQiSOxCJEJ7U1zNaOtkM261OLG4sga3OmpV/rpW3eDj69Vwaq5nfDeurnBbr1bj6bWjo3zuR468IRgr9fDtqpDWwQBQLPFj8ra3v4nGz/1XuJzzyjov59zfz0tFe5WP7yH21sVXI22Rb/AU2BtL/JhMrEdaiBfD1l2B2GcAuCGZUylpIZJHYhciESR2IRJBYhciESR2IRJBYhciESR2IRKhzaWkAbCyyZGayWbEf6xFNsW5Z9uo8bbKKyQFdmWZe7JnXzxP4/19vCVzvsBbOk9Nhb3snl7+en702GEaHxzi+214MDJ+8JVgLHZfRf8APybsvgsAKPaS5ed4qeeswfdbvcbTsbNIWfQ8uc7mCpGS6h7eLkN4XrqyC5EIErsQiSCxC5EIErsQiSCxC5EIErsQiSCxC5EIbfXZPWugurEcjJdK3PtEjviukdbCWZ21ewbK69w3nXk1nHP+8nmeGz07E95mADiwn2/3/Dwve3zx5fVgbGi4n47dWOMefyynfO4yb+lVILt9ZJSXuS4U+OlZiPjRK+vh+xOOH38zHdtbDOfCA0Aux8+nXD4SZ9uWI2XLAVis5HposTsaJYT4f4fELkQiSOxCJILELkQiSOxCJILELkQiSOxCJEJbffYsa2B9PZx7nc9xTzdfDOfxesRnr9f53NZXeW717EzYT37xuYt07G3H3kLjeeM+e3VjhcZ7i7cEY6s3wh48APzHE0/TeL0eqc3ey/3kjaWwJzw6OkbHlsu8pv3gIL9H4Pr168HYu/6AH5N6nZ8PlY0qjTcavKVzjsWNj81IPjtdZ+wJZvawmV01s+dveuxBM7tkZs80f+7d0dqFEG1jO2/jvwHgni0e/7K7n2z+PL670xJC7DZRsbv7jwHwukhCiK6nlS/oPmVmzzbf5o+GnmRmp8zsjJmdubbI7xEXQuwdOxX7VwEcB3ASwByAL4ae6O6n3X3a3acnIokPQoi9Y0did/cr7t5w9wzA1wDctbvTEkLsNjsSu5kdvOnfDwJ4PvRcIUR3EPXZzexRAO8BMGFmMwC+AOA9ZnYSgAO4AOAT21nZwnwFj/5TuIb6H/3xPjr+5J/dGowVwfOqB3q4d1mvcT/5/C9ngrFbxnlu9NUZbvIXStxPLhWGaPzSpUvhYMSzzeV5H3EDz4dfW+a9xHt7w+Mbxk+/3mGe131h7uc0fvT2kWDs2Fv4PR39I/z7pfV6uL4BABR7wusGABjJSY/UnM8Zm3t4bFTs7n7/Fg9/PTZOCNFd6HZZIRJBYhciESR2IRJBYhciESR2IRKhrSmuG5Uyzr0UtuSHRniJ3MO3hlNBB4d4e9+hYZ4OOXV4gsb3HwjbX9cuL9GxR97wJhqvNbg1t1HhKa4V0q66HsntzXHnLNpOureH24KVcrjV9egYT+1d3winqALAgcngXdoAgNtvPx6M9fZxyzGXD88bAA4dnqJxY9YawK03i7RsjsUD6MouRCJI7EIkgsQuRCJI7EIkgsQuRCJI7EIkgsQuRCK01WcfHx/FRz/2l8H48AhPO5w4Qird5CLldWvcb75liqdyHj0e9nQXF2bp2OW1cHosANTq3JO1HJ97sRTe9lyBL7tSKdP4epkb8R6xkyuVcCpoFrnWrJWv0fgth7jHf/zE4WAsl+fps8Ui97IPHgqX7wYAGPfpO4Gu7EIkgsQuRCJI7EIkgsQuRCJI7EIkgsQuRCJI7EIkQlt99v7Bfpz80zuD8cbqFb6AHuJdRnK+qxXuVQ+Pcs/2zncdDcYade5Fr0a6Xi0s8rnHWhdbPuyzW6R1cMMj7YEjra4LdX4KLa+F70HoW+blu+sZL9e8f/IAjQ8Oh49LrsDXvf/ACI3n+yLXych+37t89nBMV3YhEkFiFyIRJHYhEkFiFyIRJHYhEkFiFyIRJHYhEqGtPjuyOrK1q8FwLH95uI/42cZ901i++9AIr2H+tpPhOuGlAq+tfmWO++RLN3hOebW6TuOlnvBhrFb4ds/MLND47MwSja+v8truI+PhWKmPb9fkFK8Lf+yNvNZ/sSecs75vtI+OPXwskq/e4HOnPjoAWKT+wh4QvbKb2REz+5GZnTWzF8zs083Hx8zsCTM71/zNj4wQoqNs5218HcBn3f33APwhgE+a2R0APgfgSXc/AeDJ5v9CiC4lKnZ3n3P3p5t/rwA4C2AKwH0AHmk+7REAH9ijOQohdoHX9QWdmR0FcCeAnwKYdPc5YPMFAcCWNyqb2SkzO2NmZ+YXFlucrhBip2xb7GY2COC7AD7j7pHUjv/D3U+7+7S7T+8f08d6ITrFtsRuZkVsCv1b7v695sNXzOxgM34QQPhrdiFEx4lab2ZmAL4O4Ky7f+mm0GMAHgDwUPP3D6Jry+eQGwxbXEP5iJ1RICmuEWvNqry0rxt/szJJSgeXijw9dngft6dW17htWKvzeD4fTmusbPBUy4FhXr67v5+3wl5c4Mfs0JHwu7lSiZ9+R287SOO3HZ+k8XyxGoyNT/DS4Rjkc6te5+dLqSfWVrkF68131rJ5Oz773QA+AuA5M3um+djnsSny75jZxwG8CuBDO5qBEKItRMXu7j9BOCP+vbs7HSHEXqHbZYVIBIldiESQ2IVIBIldiESQ2IVIhPamuDYyNJbDfvfyKk8FLV+ZC8byRe5FNzLeojef5+NHhsJ+89A+kscJ4I63vYHGF5e4Z3vl6iUaX1gIpwZnxn32W6Z4eu7U4dtpPAeeKor85WCot7dEh956NNxyGQB6+7jfbPmwzz46HvHZ13jqLyL7FdZ919Hum5EQYk+Q2IVIBIldiESQ2IVIBIldiESQ2IVIBIldiERoq8/eaADLS+E83pU1nuN7YzXs0ff0R0pJG/fZyxthPxgAlq6H51bI8XJbhw+doPH9k9zznZi8lcbXy+Gc8bVVvl/W1njJ4/Ia97LLkWM2cSDcVnlomHv8E5NjNF7diOSUD4yEg/28zfbaFd4+fGCEz91rYY+/dXaWz64ruxCJILELkQgSuxCJILELkQgSuxCJILELkQgSuxCJ0FafvVbNcGU2XGe82qjT8ZmFc8rrVe73ForcVy2Q2usAsLYW9vgbtUjus1+g4d5+ntc9dSzsVQPAUH+49nupl+/TAwd5zftGjdeVzxnPZ8+XSF155x6/Z9xHzxf5tjVIrwCr8/NlYCCSpx85VzfbLYTxyLa3suwQurILkQgSuxCJILELkQgSuxCJILELkQgSuxCJILELkQjb6c9+BMA3AdyCzabSp939K2b2IIC/ATDffOrn3f1xtix3Q70a9pTdeV53oSeck97Xw33RUg/3NSt1Xgfc66QHep2/Zq6v8x7mHsm1z6p8fK5EctZzfKwVIrXXnedlG7jfDDLePVZ7nYdzuZhXHY5HrWp+WwZik2vUI/ulhWXHt3trtnNTTR3AZ939aTMbAvAzM3uiGfuyu//DjtYshGgr2+nPPgdgrvn3ipmdBTC11xMTQuwur+szu5kdBXAngJ82H/qUmT1rZg+b2Za1kczslJmdMbMzi8u8fJMQYu/YttjNbBDAdwF8xt2XAXwVwHEAJ7F55f/iVuPc/bS7T7v79OhwuFaaEGJv2ZbYzayITaF/y92/BwDufsXdG+6eAfgagLv2bppCiFaJit02U2y+DuCsu3/ppscP3vS0DwJ4fvenJ4TYLbbzbfzdAD4C4Dkze6b52OcB3G9mJ7Hpb1wA8InYgtyBygaxDXLccij2hlNce0rhGAD09ka8lgq3SjaMWEgZt5BWlss0noHbWxWSqgkAPaQ1cayEtuX4snMFbvPEHKyMpILGUjXNdp4GCvA00ixi+2WRlOlYimqjwZfPtj2f576fZ2S/kWlt59v4n2DrY0o9dSFEd6E76IRIBIldiESQ2IVIBIldiESQ2IVIBIldiERoaylp9wz1BinvG0mXzDIW55uSy3HvMubpZo3w62Ktyj3V8jr30Yu9RRqPebYMy0XG5iNedhZphR1dfyuDWyvHXCcef+yY1Wr8/oR6JIW1VOLlwZmXHrt3wWLHLICu7EIkgsQuRCJI7EIkgsQuRCJI7EIkgsQuRCJI7EIkgrXSOvZ1r8xsHsArNz00AeBa2ybw+ujWuXXrvADNbafs5txudff9WwXaKvbfWrnZGXef7tgECN06t26dF6C57ZR2zU1v44VIBIldiETotNhPd3j9jG6dW7fOC9Dcdkpb5tbRz+xCiPbR6Su7EKJNSOxCJEJHxG5m95jZL8zsJTP7XCfmEMLMLpjZc2b2jJmd6fBcHjazq2b2/E2PjZnZE2Z2rvm7Iz21AnN70MwuNffdM2Z2b4fmdsTMfmRmZ83sBTP7dPPxju47Mq+27Le2f2Y3szyAXwL4CwAzAJ4CcL+7v9jWiQQwswsApt294zdgmNm7AawC+Ka7v7X52N8DWHD3h5ovlKPu/rddMrcHAax2uo13s1vRwZvbjAP4AICPoYP7jszrr9CG/daJK/tdAF5y91+5exXAtwHc14F5dD3u/mMAC695+D4AjzT/fgSbJ0vbCcytK3D3OXd/uvn3CoBftxnv6L4j82oLnRD7FICLN/0/g+7q9+4AfmhmPzOzU52ezBZMuvscsHnyADjQ4fm8lmgb73bymjbjXbPvdtL+vFU6IfatSmx1k/93t7u/E8D7AXyy+XZVbI9ttfFuF1u0Ge8Kdtr+vFU6IfYZAEdu+v8wgNkOzGNL3H22+fsqgO+j+1pRX/l1B93m76sdns//0k1tvLdqM44u2HedbH/eCbE/BeCEmR0zsxKADwN4rAPz+C3MbKD5xQnMbADA+9B9ragfA/BA8+8HAPygg3P5DbqljXeozTg6vO863v7c3dv+A+BebH4jfx7A33ViDoF53Qbgv5s/L3R6bgAexebbuho23xF9HMA4gCcBnGv+Huuiuf0LgOcAPItNYR3s0Nz+BJsfDZ8F8Ezz595O7zsyr7bsN90uK0Qi6A46IRJBYhciESR2IRJBYhciESR2IRJBYhciESR2IRLhfwAVqdQa0VsmkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i, l in test_loader:\n",
    "    o ,mu, v = saved_model(i)\n",
    "    print(mu[0], mu[1])\n",
    "    plt.imshow(i[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(o[0].permute(1, 2, 0).detach().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e0e8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_generator(nn.Module):\n",
    "    def __init__(self, img_shape, code_size):\n",
    "        super(VAE_generator, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.C, self.H, self.W = img_shape\n",
    "        self.conv1 = nn.Conv2d(self.C, 32, kernel_size=3,  padding= \"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,  padding= \"same\")\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3,   padding= \"same\")\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding= \"same\")\n",
    "        self.fc1 = nn.Linear(256, code_size)\n",
    "        self.fc2 = nn.Linear(256, code_size)\n",
    "        \n",
    "\n",
    "        # decoder\n",
    "        self.fc3 = nn.Linear(code_size, 2304)\n",
    "        self.conv5 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv6 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "        self.conv7 = nn.ConvTranspose2d(64, self.C, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        print(mu.shape)\n",
    "        print(log_var.shape)\n",
    "        stddev = torch.exp(0.5*log_var)\n",
    "        epsilon = torch.randn_like(stddev)\n",
    "        print(stddev.shape)\n",
    "        print(epsilon.shape)\n",
    "#         print(epsilon.shape)\n",
    "        print((stddev*epsilon).shape)\n",
    "        print((mu + stddev*epsilon).shape)\n",
    "        return mu + stddev*epsilon\n",
    "    \n",
    "    def forward(self, x,z1):\n",
    "        e1 = F.elu(self.conv1(x))\n",
    "        m1 = F.max_pool2d(e1, kernel_size=2)\n",
    "        \n",
    "        e2 = F.elu(self.conv2(m1))\n",
    "        m2 = F.max_pool2d(e2, kernel_size=2)\n",
    "        \n",
    "        e3 = F.elu(self.conv3(m2))\n",
    "        m3 = F.max_pool2d(e3, kernel_size=2)\n",
    "        \n",
    "        e4 = F.elu(self.conv4(m3))\n",
    "        m4 = F.max_pool2d(e4, kernel_size=2)\n",
    "        \n",
    "        f1 = m4.view(m4.size(0), -1) \n",
    "        \n",
    "        mu = self.fc1(f1)\n",
    "        logvar = self.fc2(f1)\n",
    "        \n",
    "        z = self.sampling(mu,logvar)\n",
    "        \n",
    "        d5 = self.fc3(z1)\n",
    "        \n",
    "        d4 = d5.view(d5.size(0), 256, 3, 3)\n",
    "        d3 = F.elu(self.conv5(d4))\n",
    "        d2 = F.elu(self.conv6(d3))\n",
    "        d1 = self.conv7(d2)\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c813fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 128\n",
    "saved_model = VAE_generator(img_shape = (3,28,28),code_size  = z_dim) \n",
    "saved_model.load_state_dict(torch.load('simple_vae_pytorch_model_128_code_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e7fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# m = torch.randn(BATCH_SIZE, 32)\n",
    "# log = torch.randn(BATCH_SIZE, 32)\n",
    "\n",
    "z = np.random.normal(0, 1, size=(1, 128))\n",
    "z = torch.Tensor(z)\n",
    "# print(m.shape)\n",
    "for i, l in test_loader:\n",
    "#     test = torch.zeros((BATCH_SIZE,3,28,28))\n",
    "    o = saved_model(i, z)\n",
    "    plt.imshow(i[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(o[0].permute(1, 2, 0).detach().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30434335",
   "metadata": {},
   "source": [
    "# Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a827c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_skip(nn.Module):\n",
    "    def __init__(self, img_shape, code_size):\n",
    "        super(VAE_skip, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.C, self.H, self.W = img_shape\n",
    "        self.conv1 = nn.Conv2d(self.C, 32, kernel_size=3,  padding= \"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,  padding= \"same\")\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3,   padding= \"same\")\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding= \"same\")\n",
    "        self.fc1 = nn.Linear(256, code_size)\n",
    "        self.fc2 = nn.Linear(256, code_size)\n",
    "        \n",
    "\n",
    "        # decoder\n",
    "        self.fc3 = nn.Linear(code_size, 2304)\n",
    "        self.conv5 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv6 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "        self.conv7 = nn.ConvTranspose2d(64, self.C, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        stddev = torch.exp(0.5*log_var)\n",
    "        epsilon = torch.randn_like(stddev)\n",
    "        return mu + stddev*epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = F.elu(self.conv1(x))\n",
    "        m1 = F.max_pool2d(e1, kernel_size=2)\n",
    "        \n",
    "        e2 = F.elu(self.conv2(m1))\n",
    "        m2 = F.max_pool2d(e2, kernel_size=2)\n",
    "        \n",
    "        e3 = F.elu(self.conv3(m2))\n",
    "        m3 = F.max_pool2d(e3, kernel_size=2)\n",
    "        \n",
    "        e4 = F.elu(self.conv4(m3))\n",
    "        m4 = F.max_pool2d(e4, kernel_size=2)\n",
    "        \n",
    "        f1 = m4.view(m4.size(0), -1) \n",
    "        \n",
    "        mu = self.fc1(f1)\n",
    "        logvar = self.fc2(f1)\n",
    "        z = self.sampling(mu, logvar)\n",
    "        \n",
    "        d5 = self.fc3(z)\n",
    "        \n",
    "        d4 = d5.view(d5.size(0), 256, 3, 3)\n",
    "#         c1 = d4+e4\n",
    "        d3 = F.elu(self.conv5(d4))\n",
    "        c2 = d3+0.1*e3\n",
    "        d2 = F.elu(self.conv6(c2))\n",
    "#         c3 = d2+e2\n",
    "        d1 = self.conv7(d2)\n",
    "        return d1, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e23b3595",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 32\n",
    "vae_skip = VAE_skip(img_shape = (3,28,28),code_size  = z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ace7ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             896\n",
      "            Conv2d-2           [-1, 64, 14, 14]          18,496\n",
      "            Conv2d-3            [-1, 128, 7, 7]          73,856\n",
      "            Conv2d-4            [-1, 256, 3, 3]         295,168\n",
      "            Linear-5                   [-1, 32]           8,224\n",
      "            Linear-6                   [-1, 32]           8,224\n",
      "            Linear-7                 [-1, 2304]          76,032\n",
      "   ConvTranspose2d-8            [-1, 128, 7, 7]         295,040\n",
      "   ConvTranspose2d-9           [-1, 64, 14, 14]          73,792\n",
      "  ConvTranspose2d-10            [-1, 3, 28, 28]           1,731\n",
      "================================================================\n",
      "Total params: 851,459\n",
      "Trainable params: 851,459\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.53\n",
      "Params size (MB): 3.25\n",
      "Estimated Total Size (MB): 3.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(vae_skip.cuda(),(3,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec052ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(vae_skip.parameters())\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380ab99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_epoch(epoch_index, model):\n",
    "#     running_loss = 0.\n",
    "#     last_loss = 0.\n",
    "\n",
    "#     # Here, we use enumerate(training_loader) instead of\n",
    "#     # iter(training_loader) so that we can track the batch\n",
    "#     # index and do some intra-epoch reporting\n",
    "#     for i, data in enumerate(train_loader):\n",
    "#         # Every data instance is an input + label pair\n",
    "#         inputs, labels = data\n",
    "\n",
    "#         # Zero your gradients for every batch!\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Make predictions for this batch\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Compute the loss and its gradients\n",
    "#         loss = loss_fn(outputs, inputs)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Adjust learning weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Gather data and report\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     last_loss = running_loss / len(train_loader) # loss per batch\n",
    "#     print('Epoch {} loss: {}'.format(epoch_index, last_loss))\n",
    "\n",
    "#     return last_loss\n",
    "\n",
    "\n",
    "\n",
    "# def train(model, out_path):\n",
    "#     # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "#     epoch_number = 0\n",
    "\n",
    "#     N_EPOCHS = 80\n",
    "\n",
    "#     best_vloss = 1_000_000.\n",
    "\n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "#         # Make sure gradient tracking is on, and do a pass over the data\n",
    "#         model.train(True)\n",
    "#         avg_loss = train_one_epoch(epoch_number, model)\n",
    "\n",
    "#         # We don't need gradients on to do reporting\n",
    "#         model.train(False)\n",
    "\n",
    "#         running_vloss = 0.0\n",
    "#         for i, vdata in enumerate(val_loader):\n",
    "#             vinputs, vlabels = vdata\n",
    "#             voutputs = model(vinputs)\n",
    "#             vloss = loss_fn(voutputs, vinputs)\n",
    "#             running_vloss += vloss\n",
    "\n",
    "#         avg_vloss = running_vloss / (i + 1)\n",
    "#         print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "\n",
    "#         # Track best performance, and save the model's state\n",
    "#         if avg_vloss < best_vloss:\n",
    "#             best_vloss = avg_vloss\n",
    "#             torch.save(model.state_dict(), out_path)\n",
    "\n",
    "#         epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b49641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "Epoch 0 loss: 0.05842381544819433 recon_loss: 0.05020618997514248 kl_loss: 0.008217625705284474\n",
      "LOSS train 0.05842381544819433 valid 0.018846161663532257 recon 0.018654940649867058 kl 0.00019122038793284446\n",
      "EPOCH 2:\n",
      "Epoch 1 loss: 0.018422921813310008 recon_loss: 0.018253155212134938 kl_loss: 0.00016976659987935134\n",
      "LOSS train 0.018422921813310008 valid 0.017882980406284332 recon 0.017721423879265785 kl 0.0001615581859368831\n",
      "EPOCH 3:\n",
      "Epoch 2 loss: 0.017808140434120615 recon_loss: 0.017654153671472855 kl_loss: 0.00015398676264775942\n",
      "LOSS train 0.017808140434120615 valid 0.01752801425755024 recon 0.017354007810354233 kl 0.00017400768410880119\n",
      "EPOCH 4:\n",
      "Epoch 3 loss: 0.017388113576612654 recon_loss: 0.017229046557762036 kl_loss: 0.0001590670073884922\n",
      "LOSS train 0.017388113576612654 valid 0.017237354069948196 recon 0.01707036793231964 kl 0.0001669858756940812\n",
      "EPOCH 5:\n",
      "Epoch 4 loss: 0.017075672866904163 recon_loss: 0.016904773745583752 kl_loss: 0.00017089915191930119\n",
      "LOSS train 0.017075672866904163 valid 0.0170428603887558 recon 0.016873175278306007 kl 0.0001696832332527265\n",
      "EPOCH 6:\n",
      "Epoch 5 loss: 0.016778461973195616 recon_loss: 0.01658147163306401 kl_loss: 0.00019699033754016593\n",
      "LOSS train 0.016778461973195616 valid 0.016765771433711052 recon 0.01646990329027176 kl 0.00029586991877295077\n",
      "EPOCH 7:\n",
      "Epoch 6 loss: 0.016505065482518035 recon_loss: 0.016234596291786595 kl_loss: 0.00027046920089784704\n",
      "LOSS train 0.016505065482518035 valid 0.01615763083100319 recon 0.01585710421204567 kl 0.000300527986837551\n",
      "EPOCH 8:\n",
      "Epoch 7 loss: 0.015950470333536193 recon_loss: 0.015582808299781117 kl_loss: 0.0003676620528918423\n",
      "LOSS train 0.015950470333536193 valid 0.015477437525987625 recon 0.015131194144487381 kl 0.0003462428576312959\n",
      "EPOCH 9:\n",
      "Epoch 8 loss: 0.014625097414453144 recon_loss: 0.014080375503136278 kl_loss: 0.0005447218778275259\n",
      "LOSS train 0.014625097414453144 valid 0.014361671172082424 recon 0.013379134237766266 kl 0.0009825365850701928\n",
      "EPOCH 10:\n",
      "Epoch 9 loss: 0.012380401195626552 recon_loss: 0.011660179423091754 kl_loss: 0.0007202217661558765\n",
      "LOSS train 0.012380401195626552 valid 0.01146976463496685 recon 0.010869773104786873 kl 0.0005999901914037764\n",
      "EPOCH 11:\n",
      "Epoch 10 loss: 0.010889840331438878 recon_loss: 0.010241294777250453 kl_loss: 0.0006485455529923768\n",
      "LOSS train 0.010889840331438878 valid 0.010305789299309254 recon 0.009671075269579887 kl 0.000634713564068079\n",
      "EPOCH 12:\n",
      "Epoch 11 loss: 0.009722490527041971 recon_loss: 0.009162910052970664 kl_loss: 0.0005595804714798697\n",
      "LOSS train 0.009722490527041971 valid 0.009322514757514 recon 0.008842322044074535 kl 0.0004801939649041742\n",
      "EPOCH 13:\n",
      "Epoch 12 loss: 0.008746002083133957 recon_loss: 0.008282363484932543 kl_loss: 0.0004636385874369832\n",
      "LOSS train 0.008746002083133957 valid 0.008298788219690323 recon 0.00793437473475933 kl 0.00036441534757614136\n",
      "EPOCH 14:\n",
      "Epoch 13 loss: 0.00801208577029509 recon_loss: 0.007621075460457639 kl_loss: 0.00039101031741242193\n",
      "LOSS train 0.00801208577029509 valid 0.007781451102346182 recon 0.007434772793203592 kl 0.00034667778527364135\n",
      "EPOCH 15:\n",
      "Epoch 14 loss: 0.007530831033363938 recon_loss: 0.007153193887374173 kl_loss: 0.00037763716831599195\n",
      "LOSS train 0.007530831033363938 valid 0.007356337271630764 recon 0.007034678477793932 kl 0.0003216586774215102\n",
      "EPOCH 16:\n",
      "Epoch 15 loss: 0.007141939877876884 recon_loss: 0.006868724216872903 kl_loss: 0.0002732156533293413\n",
      "LOSS train 0.007141939877876884 valid 0.007175430655479431 recon 0.006808730773627758 kl 0.00036670020199380815\n",
      "EPOCH 17:\n",
      "Epoch 16 loss: 0.0069218522403389215 recon_loss: 0.006671558730686976 kl_loss: 0.0002502935108479929\n",
      "LOSS train 0.0069218522403389215 valid 0.006828278303146362 recon 0.006581540685147047 kl 0.0002467369777150452\n",
      "EPOCH 18:\n",
      "Epoch 17 loss: 0.006828685415500443 recon_loss: 0.006578956823158142 kl_loss: 0.0002497285910465831\n",
      "LOSS train 0.006828685415500443 valid 0.006911372300237417 recon 0.006580303888767958 kl 0.00033106785849668086\n",
      "EPOCH 19:\n",
      "Epoch 18 loss: 0.006649881693827984 recon_loss: 0.006434827567754339 kl_loss: 0.00021505412736936305\n",
      "LOSS train 0.006649881693827984 valid 0.006702807731926441 recon 0.00650115218013525 kl 0.00020165566820651293\n",
      "EPOCH 20:\n",
      "Epoch 19 loss: 0.006496249777838996 recon_loss: 0.006301140422256638 kl_loss: 0.0001951093542866396\n",
      "LOSS train 0.006496249777838996 valid 0.006577540189027786 recon 0.00633480167016387 kl 0.00024273827148135751\n",
      "EPOCH 21:\n",
      "Epoch 20 loss: 0.006436837795637038 recon_loss: 0.00623667557494179 kl_loss: 0.00020016222069524738\n",
      "LOSS train 0.006436837795637038 valid 0.0069466521963477135 recon 0.006649895571172237 kl 0.00029675703262910247\n",
      "EPOCH 22:\n",
      "Epoch 21 loss: 0.006359754525737403 recon_loss: 0.0061865285523746115 kl_loss: 0.00017322597336279203\n",
      "LOSS train 0.006359754525737403 valid 0.006422452162951231 recon 0.006239809561520815 kl 0.00018264345999341458\n",
      "EPOCH 23:\n",
      "Epoch 22 loss: 0.006273417634736389 recon_loss: 0.006098623974376345 kl_loss: 0.00017479365976202007\n",
      "LOSS train 0.006273417634736389 valid 0.006245395168662071 recon 0.006063775159418583 kl 0.00018162030028179288\n",
      "EPOCH 24:\n",
      "Epoch 23 loss: 0.006202405736437195 recon_loss: 0.006034790263957765 kl_loss: 0.00016761547183157072\n",
      "LOSS train 0.006202405736437195 valid 0.006486056838184595 recon 0.006304818205535412 kl 0.00018123815243598074\n",
      "EPOCH 25:\n",
      "Epoch 24 loss: 0.006063623254965633 recon_loss: 0.005903941770530727 kl_loss: 0.00015968148383688284\n",
      "LOSS train 0.006063623254965633 valid 0.006065081804990768 recon 0.005827872082591057 kl 0.00023720922763459384\n",
      "EPOCH 26:\n",
      "Epoch 25 loss: 0.005910168111656015 recon_loss: 0.005737540895782717 kl_loss: 0.00017262721457757846\n",
      "LOSS train 0.005910168111656015 valid 0.00590382469817996 recon 0.005741845816373825 kl 0.00016197866352740675\n",
      "EPOCH 27:\n",
      "Epoch 26 loss: 0.005755170443005962 recon_loss: 0.0055896728459031205 kl_loss: 0.00016549759839855975\n",
      "LOSS train 0.005755170443005962 valid 0.005551371723413467 recon 0.00540754571557045 kl 0.00014382562949322164\n",
      "EPOCH 28:\n",
      "Epoch 27 loss: 0.0054734110838593275 recon_loss: 0.005326559945736846 kl_loss: 0.0001468511374746226\n",
      "LOSS train 0.0054734110838593275 valid 0.005327492021024227 recon 0.005214269272983074 kl 0.00011322223872411996\n",
      "EPOCH 29:\n",
      "Epoch 28 loss: 0.005249394815134471 recon_loss: 0.00512466362473389 kl_loss: 0.00012473119040058085\n",
      "LOSS train 0.005249394815134471 valid 0.005186405498534441 recon 0.005023100879043341 kl 0.0001633039937587455\n",
      "EPOCH 30:\n",
      "Epoch 29 loss: 0.005022288198034241 recon_loss: 0.004900946808111382 kl_loss: 0.00012134139057071816\n",
      "LOSS train 0.005022288198034241 valid 0.004946736618876457 recon 0.004814425949007273 kl 0.0001323105243500322\n",
      "EPOCH 31:\n",
      "Epoch 30 loss: 0.004722516033288143 recon_loss: 0.004606916329963771 kl_loss: 0.00011559970267651259\n",
      "LOSS train 0.004722516033288143 valid 0.004561241250485182 recon 0.0044682687148451805 kl 9.297232463723049e-05\n",
      "EPOCH 32:\n",
      "Epoch 31 loss: 0.004424390443664504 recon_loss: 0.004322402895275146 kl_loss: 0.00010198755063194692\n",
      "LOSS train 0.004424390443664504 valid 0.004345185123383999 recon 0.004227413795888424 kl 0.00011777172767324373\n",
      "EPOCH 33:\n",
      "Epoch 32 loss: 0.004276041265207101 recon_loss: 0.004165319611050495 kl_loss: 0.00011072165445561763\n",
      "LOSS train 0.004276041265207101 valid 0.004257074557244778 recon 0.004143077880144119 kl 0.00011399638606235385\n",
      "EPOCH 34:\n",
      "Epoch 33 loss: 0.0040899644516510505 recon_loss: 0.003990228670610957 kl_loss: 9.973576828225018e-05\n",
      "LOSS train 0.0040899644516510505 valid 0.00409088097512722 recon 0.003979528322815895 kl 0.00011135254317196086\n",
      "EPOCH 35:\n",
      "Epoch 34 loss: 0.003957437670292103 recon_loss: 0.00386501196293441 kl_loss: 9.242570511510354e-05\n",
      "LOSS train 0.003957437670292103 valid 0.003865758888423443 recon 0.0037834232207387686 kl 8.233563130488619e-05\n",
      "EPOCH 36:\n",
      "Epoch 35 loss: 0.00382117973321615 recon_loss: 0.003728486822394986 kl_loss: 9.269291560535561e-05\n",
      "LOSS train 0.00382117973321615 valid 0.0037443633191287518 recon 0.0036654777359217405 kl 7.888585241744295e-05\n",
      "EPOCH 37:\n",
      "Epoch 36 loss: 0.0036537256825409114 recon_loss: 0.0035636226825211965 kl_loss: 9.010299937185602e-05\n",
      "LOSS train 0.0036537256825409114 valid 0.003577306168153882 recon 0.003500095335766673 kl 7.721099245827645e-05\n",
      "EPOCH 38:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 loss: 0.0034875656918252574 recon_loss: 0.003413355826361947 kl_loss: 7.420986481545149e-05\n",
      "LOSS train 0.0034875656918252574 valid 0.003467770991846919 recon 0.003375693690031767 kl 9.207743278238922e-05\n",
      "EPOCH 39:\n",
      "Epoch 38 loss: 0.0033640612175126804 recon_loss: 0.0032809632112693094 kl_loss: 8.309800689123022e-05\n",
      "LOSS train 0.0033640612175126804 valid 0.0033325273543596268 recon 0.0032350250985473394 kl 9.75022412603721e-05\n",
      "EPOCH 40:\n",
      "Epoch 39 loss: 0.003211729909446448 recon_loss: 0.003135460666508401 kl_loss: 7.626924358590582e-05\n",
      "LOSS train 0.003211729909446448 valid 0.00320149352774024 recon 0.0031392548698931932 kl 6.223803939064965e-05\n",
      "EPOCH 41:\n",
      "Epoch 40 loss: 0.003116472000702705 recon_loss: 0.003048677250985311 kl_loss: 6.779474906953472e-05\n",
      "LOSS train 0.003116472000702705 valid 0.0031264626886695623 recon 0.003064980497583747 kl 6.148184911580756e-05\n",
      "EPOCH 42:\n",
      "Epoch 41 loss: 0.0029841444426382036 recon_loss: 0.0029202090410439118 kl_loss: 6.393540191822143e-05\n",
      "LOSS train 0.0029841444426382036 valid 0.0029627718031406403 recon 0.002906807465478778 kl 5.59644031454809e-05\n",
      "EPOCH 43:\n",
      "Epoch 42 loss: 0.0028998150062836603 recon_loss: 0.002840344247102023 kl_loss: 5.9470758882625514e-05\n",
      "LOSS train 0.0028998150062836603 valid 0.0029250825755298138 recon 0.002869617659598589 kl 5.5465465266024694e-05\n",
      "EPOCH 44:\n",
      "Epoch 43 loss: 0.002821763227885104 recon_loss: 0.0027674284192720708 kl_loss: 5.433480828910371e-05\n",
      "LOSS train 0.002821763227885104 valid 0.002785417018458247 recon 0.002737767994403839 kl 4.7648973122704774e-05\n",
      "EPOCH 45:\n",
      "Epoch 44 loss: 0.0027168607674156356 recon_loss: 0.002666287468618726 kl_loss: 5.0573298472979695e-05\n",
      "LOSS train 0.0027168607674156356 valid 0.0027077558916062117 recon 0.002668116707354784 kl 3.963871859014034e-05\n",
      "EPOCH 46:\n",
      "Epoch 45 loss: 0.002625681951080691 recon_loss: 0.002583306842509096 kl_loss: 4.2375108247665904e-05\n",
      "LOSS train 0.002625681951080691 valid 0.0026165295857936144 recon 0.002576966304332018 kl 3.956356522394344e-05\n",
      "EPOCH 47:\n",
      "Epoch 46 loss: 0.0025523998503483936 recon_loss: 0.0025138215573300444 kl_loss: 3.857829238294882e-05\n",
      "LOSS train 0.0025523998503483936 valid 0.002522863447666168 recon 0.002487668301910162 kl 3.51950220647268e-05\n",
      "EPOCH 48:\n",
      "Epoch 47 loss: 0.002478454636779856 recon_loss: 0.0024373430092117353 kl_loss: 4.111162756812083e-05\n",
      "LOSS train 0.002478454636779856 valid 0.0026427973061800003 recon 0.0025951373390853405 kl 4.765994526678696e-05\n",
      "EPOCH 49:\n",
      "Epoch 48 loss: 0.002399374402370559 recon_loss: 0.0023626555937457166 kl_loss: 3.6718808624842395e-05\n",
      "LOSS train 0.002399374402370559 valid 0.002374648815020919 recon 0.0023404760286211967 kl 3.417296102270484e-05\n",
      "EPOCH 50:\n",
      "Epoch 49 loss: 0.0023325607914171397 recon_loss: 0.002301782042693908 kl_loss: 3.077874904716138e-05\n",
      "LOSS train 0.0023325607914171397 valid 0.002324038650840521 recon 0.0022951862774789333 kl 2.8852326067863032e-05\n",
      "EPOCH 51:\n",
      "Epoch 50 loss: 0.0022619024895080557 recon_loss: 0.002230170543292818 kl_loss: 3.173194621523766e-05\n",
      "LOSS train 0.0022619024895080557 valid 0.0022382899187505245 recon 0.002217707224190235 kl 2.058314385067206e-05\n",
      "EPOCH 52:\n",
      "Epoch 51 loss: 0.0021965348134646576 recon_loss: 0.0021708477846640226 kl_loss: 2.5687029921929734e-05\n",
      "LOSS train 0.0021965348134646576 valid 0.0021992975380271673 recon 0.002166998339816928 kl 3.229954018024728e-05\n",
      "EPOCH 53:\n",
      "Epoch 52 loss: 0.00213316414897547 recon_loss: 0.0021065977890018933 kl_loss: 2.6566359176211162e-05\n",
      "LOSS train 0.00213316414897547 valid 0.0021610786207020283 recon 0.002139877527952194 kl 2.1201145500526763e-05\n",
      "EPOCH 54:\n",
      "Epoch 53 loss: 0.0020709811743310565 recon_loss: 0.0020480428787254513 kl_loss: 2.2938296402970405e-05\n",
      "LOSS train 0.0020709811743310565 valid 0.0021331750322133303 recon 0.002115565584972501 kl 1.7609072529012337e-05\n",
      "EPOCH 55:\n",
      "Epoch 54 loss: 0.002016173666087936 recon_loss: 0.001989859440969941 kl_loss: 2.6314228456961755e-05\n",
      "LOSS train 0.002016173666087936 valid 0.0019911606796085835 recon 0.0019719547126442194 kl 1.9206348952138796e-05\n",
      "EPOCH 56:\n",
      "Epoch 55 loss: 0.0019462974263635808 recon_loss: 0.001927362946308639 kl_loss: 1.8934476865481024e-05\n",
      "LOSS train 0.0019462974263635808 valid 0.0019325301982462406 recon 0.0019143057288601995 kl 1.8224460291094147e-05\n",
      "EPOCH 57:\n",
      "Epoch 56 loss: 0.0018923665502121392 recon_loss: 0.001871150306725798 kl_loss: 2.1216242688975922e-05\n",
      "LOSS train 0.0018923665502121392 valid 0.0018760308157652617 recon 0.0018480501603335142 kl 2.798082641675137e-05\n",
      "EPOCH 58:\n",
      "Epoch 57 loss: 0.0018299441642689277 recon_loss: 0.0018107877591344184 kl_loss: 1.915640466107368e-05\n",
      "LOSS train 0.0018299441642689277 valid 0.0018318051006644964 recon 0.0018082631286233664 kl 2.354189746256452e-05\n",
      "EPOCH 59:\n",
      "Epoch 58 loss: 0.0017703096852765407 recon_loss: 0.0017500634836224353 kl_loss: 2.024620005937472e-05\n",
      "LOSS train 0.0017703096852765407 valid 0.0017754361033439636 recon 0.001746696070767939 kl 2.874008714570664e-05\n",
      "EPOCH 60:\n",
      "Epoch 59 loss: 0.0017110692681617115 recon_loss: 0.0016923622221826282 kl_loss: 1.8707045343682994e-05\n",
      "LOSS train 0.0017110692681617115 valid 0.0016999858198687434 recon 0.0016810166416689754 kl 1.8969198208651505e-05\n",
      "EPOCH 61:\n",
      "Epoch 60 loss: 0.0016574099897534575 recon_loss: 0.0016377043291569164 kl_loss: 1.970566043457613e-05\n",
      "LOSS train 0.0016574099897534575 valid 0.0016588913276791573 recon 0.0016454692231491208 kl 1.3422009033092763e-05\n",
      "EPOCH 62:\n",
      "Epoch 61 loss: 0.0015958573020139887 recon_loss: 0.001582648290149 kl_loss: 1.3209011864988772e-05\n",
      "LOSS train 0.0015958573020139887 valid 0.0015869763446971774 recon 0.0015781125985085964 kl 8.863646144163795e-06\n",
      "EPOCH 63:\n",
      "Epoch 62 loss: 0.0015369359163044352 recon_loss: 0.001523991376927046 kl_loss: 1.2944539215424488e-05\n",
      "LOSS train 0.0015369359163044352 valid 0.0015888588968664408 recon 0.0015696296468377113 kl 1.92292809515493e-05\n",
      "EPOCH 64:\n",
      "Epoch 63 loss: 0.0014867361575964684 recon_loss: 0.0014778099920638927 kl_loss: 8.926165532575894e-06\n",
      "LOSS train 0.0014867361575964684 valid 0.0014987215399742126 recon 0.001483067055232823 kl 1.5654351955163293e-05\n",
      "EPOCH 65:\n",
      "Epoch 64 loss: 0.0014442802823072122 recon_loss: 0.0014299294689657446 kl_loss: 1.4350813341467349e-05\n",
      "LOSS train 0.0014442802823072122 valid 0.0014523888239637017 recon 0.001430479227565229 kl 2.190966006310191e-05\n",
      "EPOCH 66:\n",
      "Epoch 65 loss: 0.001406643990975843 recon_loss: 0.0013929691009402071 kl_loss: 1.3674890035635804e-05\n",
      "LOSS train 0.001406643990975843 valid 0.00140065245795995 recon 0.001395668601617217 kl 4.983872258890187e-06\n",
      "EPOCH 67:\n",
      "Epoch 66 loss: 0.001356390773714201 recon_loss: 0.0013490968929246476 kl_loss: 7.29388110725367e-06\n",
      "LOSS train 0.001356390773714201 valid 0.0013788100332021713 recon 0.0013603151310235262 kl 1.8494878531782888e-05\n",
      "EPOCH 68:\n",
      "Epoch 67 loss: 0.0013216546838720367 recon_loss: 0.0013091020144152213 kl_loss: 1.2552669294850664e-05\n",
      "LOSS train 0.0013216546838720367 valid 0.001329930149950087 recon 0.0013200544053688645 kl 9.875751857180148e-06\n",
      "EPOCH 69:\n",
      "Epoch 68 loss: 0.0012877489279075334 recon_loss: 0.0012804278346378202 kl_loss: 7.3210935936427405e-06\n",
      "LOSS train 0.0012877489279075334 valid 0.0012999344617128372 recon 0.001292788190767169 kl 7.146321422624169e-06\n",
      "EPOCH 70:\n",
      "Epoch 69 loss: 0.0012567490769856391 recon_loss: 0.0012471909795836737 kl_loss: 9.55809724311546e-06\n",
      "LOSS train 0.0012567490769856391 valid 0.0012650220887735486 recon 0.0012594300787895918 kl 5.592049546976341e-06\n",
      "EPOCH 71:\n",
      "Epoch 70 loss: 0.0012329970725671682 recon_loss: 0.001225396592093453 kl_loss: 7.600480156014944e-06\n",
      "LOSS train 0.0012329970725671682 valid 0.0012365387519821525 recon 0.0012292253086343408 kl 7.313597961911e-06\n",
      "EPOCH 72:\n",
      "Epoch 71 loss: 0.0011974204601145873 recon_loss: 0.0011849338204714737 kl_loss: 1.2486639801963601e-05\n",
      "LOSS train 0.0011974204601145873 valid 0.001202925923280418 recon 0.0011993967927992344 kl 3.5290711366542382e-06\n",
      "EPOCH 73:\n",
      "Epoch 72 loss: 0.0011740278210235786 recon_loss: 0.0011717164649768439 kl_loss: 2.3113563659921653e-06\n",
      "LOSS train 0.0011740278210235786 valid 0.001180098275654018 recon 0.0011784059461206198 kl 1.6924552710406715e-06\n",
      "EPOCH 74:\n",
      "Epoch 73 loss: 0.0011518939409392234 recon_loss: 0.0011483138045997755 kl_loss: 3.58013665714829e-06\n",
      "LOSS train 0.0011518939409392234 valid 0.0011755491141229868 recon 0.001164302695542574 kl 1.1246392205066513e-05\n",
      "EPOCH 75:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 loss: 0.0011288094428393428 recon_loss: 0.001118785594845165 kl_loss: 1.0023847835327615e-05\n",
      "LOSS train 0.0011288094428393428 valid 0.0011913103517144918 recon 0.0011824530083686113 kl 8.857403372530825e-06\n",
      "EPOCH 76:\n",
      "Epoch 75 loss: 0.0011190139211727027 recon_loss: 0.0011087320434018868 kl_loss: 1.028187777081581e-05\n",
      "LOSS train 0.0011190139211727027 valid 0.00114650116302073 recon 0.0011344596277922392 kl 1.204155068990076e-05\n",
      "EPOCH 77:\n",
      "Epoch 76 loss: 0.0010993159120927935 recon_loss: 0.0010909251342463779 kl_loss: 8.390777522485946e-06\n",
      "LOSS train 0.0010993159120927935 valid 0.00119247124530375 recon 0.0011697569862008095 kl 2.2714404622092843e-05\n",
      "EPOCH 78:\n",
      "Epoch 77 loss: 0.0010859994496831237 recon_loss: 0.001076444296950908 kl_loss: 9.555152732215515e-06\n",
      "LOSS train 0.0010859994496831237 valid 0.0011077610542997718 recon 0.0010931181022897363 kl 1.4643004760728218e-05\n",
      "EPOCH 79:\n",
      "Epoch 78 loss: 0.0010691662155544665 recon_loss: 0.0010590459657067509 kl_loss: 1.0120251202613572e-05\n",
      "LOSS train 0.0010691662155544665 valid 0.001087035983800888 recon 0.0010711951181292534 kl 1.5840780179132707e-05\n",
      "EPOCH 80:\n",
      "Epoch 79 loss: 0.0010525767060717542 recon_loss: 0.0010436576996827882 kl_loss: 8.919005828318728e-06\n",
      "LOSS train 0.0010525767060717542 valid 0.0010935618774965405 recon 0.001080890535376966 kl 1.26713694044156e-05\n"
     ]
    }
   ],
   "source": [
    "train_vae(vae_skip, 'skip_vae_pytorch_model_kl_skip_1layer_0.1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09b4a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = VAE_skip(img_shape = (3,28,28),code_size  = z_dim) \n",
    "saved_model.load_state_dict(torch.load('skip_vae_pytorch_model_kl_skip_1layer_0.1_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5410e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS test 0.001068284036591649\n"
     ]
    }
   ],
   "source": [
    "running_tloss = 0.0\n",
    "for i, tdata in enumerate(test_loader):\n",
    "    tinputs, tlabels = tdata\n",
    "    toutputs, tmu, tlog_var = saved_model(tinputs)\n",
    "    \n",
    "    t_recon_loss = loss_fn(toutputs, tinputs)\n",
    "    t_kl_loss = KLDivLoss(tmu, tlog_var)\n",
    "    tloss = t_recon_loss + t_kl_loss\n",
    "    running_tloss += t_recon_loss\n",
    "\n",
    "\n",
    "avg_tloss = running_tloss / (i + 1)\n",
    "print('LOSS test {}'.format(avg_tloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56018cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i, l in test_loader:\n",
    "    o, mu, log = saved_model(i)\n",
    "    plt.imshow(i[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(o[0].permute(1, 2, 0).detach().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40af7d7",
   "metadata": {},
   "source": [
    "# Random image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c9d56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_skip_generator(nn.Module):\n",
    "    def __init__(self, img_shape, code_size):\n",
    "        super(VAE_skip_generator, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.C, self.H, self.W = img_shape\n",
    "        self.conv1 = nn.Conv2d(self.C, 32, kernel_size=3,  padding= \"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,  padding= \"same\")\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3,   padding= \"same\")\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding= \"same\")\n",
    "        self.fc1 = nn.Linear(256, code_size)\n",
    "        self.fc2 = nn.Linear(256, code_size)\n",
    "        \n",
    "\n",
    "        # decoder\n",
    "        self.fc3 = nn.Linear(code_size, 2304)\n",
    "        self.conv5 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv6 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "        self.conv7 = nn.ConvTranspose2d(64, self.C, kernel_size=3, stride=2, padding=1, output_padding = 1)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        stddev = torch.exp(0.5*log_var)\n",
    "        epsilon = torch.randn_like(stddev)\n",
    "        return mu + stddev*epsilon\n",
    "    \n",
    "    def forward(self, x, z1):\n",
    "        e1 = F.elu(self.conv1(x))\n",
    "        m1 = F.max_pool2d(e1, kernel_size=2)\n",
    "        \n",
    "        e2 = F.elu(self.conv2(m1))\n",
    "        m2 = F.max_pool2d(e2, kernel_size=2)\n",
    "        \n",
    "        e3 = F.elu(self.conv3(m2))\n",
    "        m3 = F.max_pool2d(e3, kernel_size=2)\n",
    "        \n",
    "        e4 = F.elu(self.conv4(m3))\n",
    "        m4 = F.max_pool2d(e4, kernel_size=2)\n",
    "        \n",
    "        f1 = m4.view(m4.size(0), -1) \n",
    "        \n",
    "        mu = self.fc1(f1)\n",
    "        logvar = self.fc2(f1)\n",
    "        z = self.sampling(mu, logvar)\n",
    "        \n",
    "#         z = self.sampling(z1, z2)\n",
    "        \n",
    "        d5 = self.fc3(z1)\n",
    "        \n",
    "        d4 = d5.view(d5.size(0), 256, 3, 3)\n",
    "#         c1 = d4+e4\n",
    "        d3 = F.elu(self.conv5(d4))\n",
    "#         c2 = d3 +0.3*e3\n",
    "        d2 = F.elu(self.conv6(d3))\n",
    "#         c3 = d2+e2\n",
    "        d1 = self.conv7(d2)\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf1cead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim= 32\n",
    "saved_model = VAE_skip_generator(img_shape = (3,28,28),code_size  = z_dim) \n",
    "saved_model.load_state_dict(torch.load('skip_vae_pytorch_model_kl_skip_1layer_0.1_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(saved_model.cuda(),[(3,28,28),(1,32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30cf554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS test 0.11207801848649979\n"
     ]
    }
   ],
   "source": [
    "z = np.random.normal(0, 1, size=(1, 32))\n",
    "z = torch.Tensor(z)\n",
    "running_tloss = 0.0\n",
    "for i, tdata in enumerate(test_loader):\n",
    "    tinputs, tlabels = tdata\n",
    "    toutputs = saved_model(tinputs, z)\n",
    "\n",
    "    t_recon_loss = loss_fn(toutputs, tinputs)\n",
    "#     t_kl_loss = KLDivLoss(tmu, tlog_var)\n",
    "#     tloss = t_recon_loss + t_kl_loss\n",
    "    running_tloss += t_recon_loss\n",
    "\n",
    "\n",
    "avg_tloss = running_tloss / (i + 1)\n",
    "print('LOSS test {}'.format(avg_tloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.random.normal(0, 1, size=(16, 32))\n",
    "z = torch.Tensor(z)\n",
    "# img = torch.zeros(())\n",
    "\n",
    "for i, l in train_loader:\n",
    "    test = torch.zeros((BATCH_SIZE,3,28,28))\n",
    "    o = saved_model(test,z)\n",
    "    print(o.shape)\n",
    "#     plt.show()\n",
    "    plt.imshow(test[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(o[0].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    print(i[0])\n",
    "    print(o[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86610e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
